---
title: "Principal Component Analysis"
output:
  html_document:
    code_folding: show
    toc: no
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
bibliography: refs.bib
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, fig.width = 5, fig.height = 4, fig.align = 'center')
```


Source: James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013).
**An introduction to statistical learning, volume 6.**
  *Springer.*: 407-409.

Using the data set `USArrests`, included as part of the `R base` package, we are going to perform Principal Component Analysis. 

The rows of the data set contain the fifty states, in alphabetical order.

```{r}
states<- row.names(USArrests)
states
```

The columns of the data set contain the four variables.
```{r}
names(USArrests)
```

We can examine the data with `summary`.
```{r}
summary(USArrests)
```
Note that there are on average three times as many rapes as murders and more than eight times as many assaults as rapes.

We can also check the variances of the variables.
```{r}
apply(USArrests, 2, var)
```
They also have very different variances.
The broad ranges of means and variances among the variables are not surprising: the `UrbanPop` variable measures the percentage of the population in each state living in an urban area, which is not a comparable number to
the number of rapes in each state per 100,000 individuals. If we failed to
scale the variables before performing PCA, then most of the principal components that we observed would be driven by the `Assault` variable, since
it has by far the largest mean and variance. Thus, it is important to standardize the variables to have mean zero and standard deviation one before
performing PCA.

We now perform principal components analysis using the function `prcomp`, one of several functions to perform PCA in `R`.
```{r}
pr.out <- prcomp ( USArrests , scale = TRUE )
```
By default, the `prcomp` function centers the variables to have mean zero.
By using the option `scale=TRUE` , we scale the variables to have standard deviation one. The output from `prcomp` contains a number of useful quantities.
```{r}
names(pr.out)
```
The `center` and `scale` components correspond to the means and standard deviations of the variables that were used for scaling prior to implementing PCA.
```{r}
pr.out$center
```
```{r}
pr.out$scale
```

The rotation matrix provides the principal component loadings; each column of `pr.out$rotation` contains the corresponding principal component loading vector. This function names it the rotation matrix, because when we matrix-multiply the
$X$ matrix by `pr.out$rotation` , it gives us the coordinates of the data in the rotated
coordinate system. These coordinates are the principal component scores.
```{r}
pr.out$rotation
```

We see that there are four distinct principal components. This is to be expected because there are in general $\min(N-1, p)$ informative principal components in a data set with $N$ observations and $p$ variables.

Using the `prcomp` function, we do not need to explicitly multiply the data by the principal component loading vectors in order to obtain the principal component score vectors. Rather the $50 \times 4$ matrix $x$ has as its columns the principal component score vectors. That is, the $k$-th column is the $k$-th principal component score vector.
```{r}
dim(pr.out$x)
```

We can plot the first two principal components as follows,
```{r, fig.width=6, fig.height=6}
biplot(pr.out, scale = 0, cex=0.8)
```
The `scale=0` argument to `biplot` ensures that the arrows are scaled to represent the loadings; other values for `scale` give slightly different biplots with different interpretations.

The `prcomp` function also outputs the standard deviation of each principal component. For instance, on the `USArrests` data set, we can access these standard deviations as follows,
```{r}
pr.out$sdev
```

The variance explained by each principal component is obtained by squaring these
```{r}
pr.out$sdev^2
```

To compute the proportion of variance explained by each principal component, we simply divide the variance explained by each principal component by the total variance explained by all four principal components.
```{r}
pve <- pr.out$sdev^2/sum(pr.out$sdev^2); pve
```

We see that the first principal component explains 62.0% of the variance in the data, the next principal component explains 24.7% of the variance, and so forth. We can plot the PVE explained by each component, as well as the cumulative PVE, as follows,
```{r, fig.width=5, fig.height=5}
plot( pve, xlab ="Principal Component", ylab ="Proportion of
Variance Explained", ylim = c (0 ,1), type = 'b')
```
```{r, fig.height=5, fig.width=5}
plot( cumsum( pve ), xlab ="Principal Component", ylab="Cumulative Proportion of Variance Explained ", ylim = c (0 ,1) ,type = 'b')
```